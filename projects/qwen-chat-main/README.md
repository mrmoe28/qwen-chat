# ğŸ¤– Qwen Chat Interface

<div align="center">

**Modern, feature-rich chat interface for Qwen AI with document processing and RAG capabilities**

![Version](https://img.shields.io/badge/version-0.1.0-blue?style=for-the-badge)
![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)
![Next.js](https://img.shields.io/badge/Next.js-16.0.1-black?style=for-the-badge&logo=next.js)
![React](https://img.shields.io/badge/React-19.2.0-61dafb?style=for-the-badge&logo=react)

*A powerful chat interface combining Qwen 2.5 with Ollama, featuring document processing, web search, and an elegant UI built with Next.js 15 and React 19*

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [Documentation](#-usage) â€¢ [Examples](#-examples)

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’» Usage](#-usage)
- [ğŸ“„ Document Processing](#-document-processing)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ“± Mobile Support](#-mobile-support)
- [ğŸ—ï¸ Project Structure](#ï¸-project-structure)
- [ğŸ”§ Development](#-development)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“œ License](#-license)

---

## âœ¨ Features

### Core Capabilities

- ğŸ¤– **Qwen 2.5 Integration** - Powered by Ollama with qwen2.5-coder:7b model
- ğŸ’¬ **Real-time Streaming** - See AI responses as they're generated
- ğŸ“„ **Document Processing** - Upload and analyze PDFs, DOCX, TXT, and Markdown files
- ğŸ” **Web Search** - Integrated DuckDuckGo search for current information
- ğŸŒ **Webpage Extraction** - Extract and analyze content from any URL
- ğŸ§  **RAG (Retrieval-Augmented Generation)** - Vector database for document search
- ğŸ’¾ **Conversation Memory** - Persistent storage with ChromaDB

### User Experience

- ğŸ¨ **Beautiful UI** - Modern design with ShadCN UI components and Tailwind CSS
- ğŸ“± **Mobile Optimized** - Responsive design with mobile-specific features
- ğŸŒ“ **Dark Mode** - Automatic dark/light mode with system preference detection
- ğŸ“Š **Multiple Conversations** - Organize chats in projects with sidebar navigation
- âš¡ **Instant Actions** - Floating action button for quick new chats on mobile
- ğŸ”„ **Auto-save** - All conversations automatically saved to localStorage

### Advanced Features

- ğŸ–¼ï¸ **Multimodal Support** - Attach images and documents to messages
- ğŸ¯ **Smart Model Selection** - Automatic model selection based on task type
- ğŸ”Š **Text-to-Speech (TTS)** - Browser-based voice output using system voices (macOS Samantha, Windows Jenny)
- ğŸ“ˆ **Performance Optimized** - Hardware-accelerated embeddings with Apple Silicon support
- ğŸ”§ **Customizable Settings** - Adjust temperature, tokens, and other model parameters
- ğŸ“– **Markdown Rendering** - Rich text formatting with code highlighting

### What Makes It Different?

| Feature | Qwen Chat | Traditional Chatbots |
|---------|-----------|---------------------|
| Document Processing | âœ… PDF, DOCX, TXT, MD | âŒ Limited |
| Mobile UX | âœ… Floating buttons, optimized UI | âŒ Desktop only |
| RAG Integration | âœ… Built-in vector DB | âŒ None |
| Local AI | âœ… 100% offline capable | âŒ Cloud-dependent |
| Streaming | âœ… Real-time responses | âš ï¸ Some |
| Projects | âœ… Organize conversations | âŒ Single thread |

---

## ğŸ“¦ Installation

### Prerequisites

- **Node.js** 18+ or higher
- **Python** 3.10+ (for backend)
- **Ollama** with Qwen 2.5 model installed
- **npm** or **yarn** or **pnpm**

### Quick Install

#### 1. Clone the Repository

```bash
git clone https://github.com/mrmoe28/qwen-chat.git
cd qwen-chat
```

#### 2. Install Ollama & Qwen Model

```bash
# Install Ollama (macOS)
brew install ollama

# Pull Qwen model
ollama pull qwen2.5-coder:7b
ollama pull qwen2.5:7b

# Start Ollama server
ollama serve
```

#### 3. Install Frontend Dependencies

```bash
cd frontend
npm install
```

#### 4. Install Backend Dependencies

```bash
cd ../backend
pip install -r requirements.txt
```

---

## ğŸš€ Quick Start

Get started in less than 5 minutes:

### 1. Start the Backend

```bash
cd backend
python3 main.py
```

**Expected output:**
```
ğŸš€ Starting Qwen Chat Backend (Ollama Edition)...
âœ… Connected to Ollama at http://localhost:11434
ğŸ“¦ Available models: ['qwen2.5-coder:7b', 'qwen2.5:7b']
âœ… Backend ready!
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 2. Start the Frontend

```bash
cd frontend
npm run dev
```

**Expected output:**
```
â–² Next.js 16.0.1
- Local:        http://localhost:3000
- Network:      http://0.0.0.0:3000
âœ“ Ready in 2.5s
```

### 3. Open Your Browser

Visit [http://localhost:3000](http://localhost:3000)

**That's it!** ğŸ‰ You're ready to chat with Qwen.

---

## ğŸ’» Usage

### Basic Chat

1. Click **"New conversation"** in the sidebar
2. Type your message in the input field
3. Press **Enter** or click **Send**
4. Watch the AI respond in real-time

### Document Upload & Analysis

#### Uploading Documents

```markdown
1. Click the ğŸ“ paperclip icon
2. Select a PDF, DOCX, TXT, or MD file
3. Type your question about the document
4. Send the message
```

#### Supported Formats

- **PDF** (.pdf) - Extracted using pypdf
- **Word** (.docx) - Parsed with python-docx
- **Text** (.txt) - Direct reading
- **Markdown** (.md) - Converted to text

#### Example: Analyzing a PDF

```
Upload: electric_bill.pdf
Message: "What are the total charges on this bill?"

Qwen will:
âœ“ Extract all text from the PDF
âœ“ Analyze the charges
âœ“ Calculate totals
âœ“ Explain the breakdown
```

### Advanced Features

#### Web Search

```
Message: "What's the latest news about AI?"

Qwen automatically:
âœ“ Searches DuckDuckGo
âœ“ Fetches relevant pages
âœ“ Synthesizes information
âœ“ Provides sourced answers
```

#### Project Organization

1. Click **Menu** â†’ **New Project**
2. Enter project name and description
3. Create conversations within projects
4. All conversations organized by project

#### Model Settings

Adjust AI behavior:

- **Temperature** (0-2): Creativity level
- **Max Tokens** (256-8192): Response length
- **Top P** (0-1): Sampling diversity
- **Frequency/Presence Penalty**: Repetition control

---

## ğŸ“„ Document Processing

### How It Works

```mermaid
graph LR
    A[Upload Document] --> B[Extract Text]
    B --> C[Process Content]
    C --> D[Include in Context]
    D --> E[AI Analysis]
```

### API Endpoint

```typescript
POST /api/process-document

// Request
FormData: {
  file: File (PDF, DOCX, TXT, MD)
}

// Response
{
  "success": true,
  "filename": "document.pdf",
  "text": "Extracted content...",
  "chunks": 5,
  "size": 12543
}
```

### Processing Pipeline

1. **Upload** - File sent to backend
2. **Extract** - Text extracted using appropriate parser
3. **Chunk** - Split into manageable pieces (1000 chars)
4. **Context** - Included in chat message
5. **Analyze** - AI processes the content

---

## âš™ï¸ Configuration

### Environment Variables

#### Backend Environment

Create a `.env` file in the backend directory:

```bash
# .env (backend)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:7b
DATABASE_PATH=./data/conversations.db
LOG_LEVEL=INFO
```

#### Frontend Environment

Create a `frontend/.env.local` file for environment-specific configuration:

**For Local Development:**
```bash
# frontend/.env.local
NEXT_PUBLIC_API_BASE=http://localhost:8000
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
```

**For Production / Remote Access (via Cloudflare Tunnel):**
```bash
# frontend/.env.local
NEXT_PUBLIC_API_BASE=https://api.mrqwen.us
NEXT_PUBLIC_BACKEND_URL=https://api.mrqwen.us
```

> **Note:** The `.env.local` file is gitignored and must be created manually for each environment.

### Frontend Configuration

Update `frontend/src/lib/qwen-api.ts`:

```typescript
// API Base URL
const API_BASE_URL = '/api';

// Default settings
export const DEFAULT_MODEL_SETTINGS: ModelSettings = {
  temperature: 0.7,
  maxTokens: 2048,
  topP: 0.9,
  frequencyPenalty: 0,
  presencePenalty: 0,
};
```

### Backend Configuration

Update `backend/main.py`:

```python
# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "qwen2.5-coder:7b"
```

---

## ğŸ“± Mobile Support

### Mobile-Specific Features

#### Floating Action Button (FAB)

- **Location**: Bottom-right corner (mobile only)
- **Function**: Quick new conversation
- **Always accessible** - No scrolling needed

#### Responsive Design

- âœ… Touch-optimized UI
- âœ… Mobile-friendly input
- âœ… Hamburger menu navigation
- âœ… Swipe-friendly sidebar

#### Scroll Improvements

- Tap **Qwen2.5** title to scroll to top
- Natural scrolling in conversations
- Auto-scroll to latest message

### Mobile Access via Ngrok

```bash
# Install ngrok
brew install ngrok

# Tunnel local server
ngrok http 3000

# Access from mobile
https://your-subdomain.ngrok-free.app
```

---

## ğŸ—ï¸ Project Structure

```
qwen-chat/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx            # Main page
â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css         # Global styles
â”‚   â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚   â”‚       â””â”€â”€ chat/
â”‚   â”‚   â”‚           â””â”€â”€ route.ts    # API proxy
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                 # ShadCN components
â”‚   â”‚   â”‚   â”œâ”€â”€ chat-interface.tsx  # Main chat UI
â”‚   â”‚   â”‚   â”œâ”€â”€ settings-dialog.tsx # Settings panel
â”‚   â”‚   â”‚   â”œâ”€â”€ artifact-preview.tsx # Code/artifact viewer
â”‚   â”‚   â”‚   â””â”€â”€ markdown-message.tsx # Message renderer
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ qwen-api.ts         # API client
â”‚   â”‚   â”‚   â”œâ”€â”€ storage.ts          # localStorage
â”‚   â”‚   â”‚   â””â”€â”€ utils.ts            # Utilities
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ chat.ts             # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                     # FastAPI server
â”‚   â”œâ”€â”€ requirements.txt            # Python deps
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ memory_service.py       # ChromaDB integration
â”‚   â”‚   â”œâ”€â”€ document_processor.py   # PDF/DOCX processing
â”‚   â”‚   â”œâ”€â”€ web_search.py           # DuckDuckGo search
â”‚   â”‚   â”œâ”€â”€ optimized_embeddings.py # Vector embeddings
â”‚   â”‚   â””â”€â”€ model_registry.py       # Model management
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ conversations.db        # SQLite database
â””â”€â”€ README.md
```

---

## ğŸ”§ Development

### Run Development Server

```bash
# Frontend (with hot reload)
cd frontend
npm run dev

# Backend (with auto-restart)
cd backend
python3 main.py
```

### Build for Production

```bash
# Frontend
cd frontend
npm run build
npm start

# Backend
cd backend
python3 main.py
```

### Run Linter

```bash
# Frontend
cd frontend
npm run lint

# Fix issues automatically
npm run lint -- --fix
```

### Testing

```bash
# Frontend (when tests are added)
npm test

# Backend
pytest
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### Error: "Ollama not responding"

**Cause:** Ollama server not running

**Solution:**
```bash
# Start Ollama
ollama serve

# Check if running
curl http://localhost:11434/api/tags
```

#### Error: "Module not found: pypdf"

**Cause:** Backend dependencies not installed

**Solution:**
```bash
cd backend
pip install -r requirements.txt
```

#### Error: "Port 3000 already in use"

**Cause:** Another process using port 3000

**Solution:**
```bash
# Find process
lsof -i :3000

# Kill process
kill -9 <PID>

# Or use different port
PORT=3001 npm run dev
```

#### Mobile: Can't scroll to top

**Solution:**
- Tap the **Qwen2.5** title in header
- Or use the **+** floating button (bottom-right)

### Need More Help?

- ğŸ“– [Full Documentation](https://github.com/mrmoe28/qwen-chat/wiki)
- ğŸ’¬ [Discussions](https://github.com/mrmoe28/qwen-chat/discussions)
- ğŸ› [Report Bug](https://github.com/mrmoe28/qwen-chat/issues)

---

## ğŸ“š Examples

### Example 1: Document Analysis

```markdown
1. Upload "contract.pdf"
2. Ask: "What are the key terms and obligations?"
3. Qwen extracts text and analyzes:
   âœ“ Identifies parties
   âœ“ Lists obligations
   âœ“ Highlights important dates
   âœ“ Summarizes terms
```

### Example 2: Code Assistance

```markdown
Message: "Write a Python function to calculate fibonacci numbers"

Response:
```python
def fibonacci(n: int) -> int:
    """Calculate the nth Fibonacci number."""
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
```
```

### Example 3: Web Research

```markdown
Message: "What are the latest developments in quantum computing?"

Qwen will:
1. Search DuckDuckGo
2. Fetch relevant articles
3. Synthesize information
4. Provide sourced summary
```

---

## ğŸ› ï¸ Technologies Used

### Frontend

- **[Next.js 16.0.1](https://nextjs.org/)** - React framework with App Router
- **[React 19.2.0](https://react.dev/)** - UI library with React Server Components
- **[TypeScript 5](https://www.typescriptlang.org/)** - Type safety
- **[Tailwind CSS 4](https://tailwindcss.com/)** - Utility-first styling
- **[ShadCN UI](https://ui.shadcn.com/)** - Component library
- **[Lucide React](https://lucide.dev/)** - Beautiful icons
- **[React Markdown](https://github.com/remarkjs/react-markdown)** - Markdown rendering

### Backend

- **[FastAPI](https://fastapi.tiangolo.com/)** - High-performance async Python framework
- **[Ollama](https://ollama.ai/)** - Local LLM server
- **[ChromaDB](https://www.trychroma.com/)** - Vector database for RAG
- **[pypdf](https://pypdf.readthedocs.io/)** - PDF text extraction
- **[python-docx](https://python-docx.readthedocs.io/)** - DOCX processing
- **[sentence-transformers](https://www.sbert.net/)** - Text embeddings
- **[MLX](https://github.com/ml-explore/mlx)** - Apple Silicon acceleration

---

## ğŸ—ºï¸ Roadmap

### Current Version (0.1.0)
- âœ… Core chat functionality
- âœ… Document processing (PDF, DOCX, TXT, MD)
- âœ… Mobile optimization
- âœ… RAG integration
- âœ… Project organization

### Next Release (0.2.0)
- âœ… Text-to-Speech (TTS) for AI responses
- ğŸ”„ Voice input (STT)
- ğŸ”„ Enhanced artifact viewer
- ğŸ”„ Advanced search filters
- ğŸ“‹ Export conversations
- ğŸ“‹ Custom system prompts

### Future
- [ ] Multi-language support
- [ ] Collaborative features
- [ ] API key management UI
- [ ] Plugin system
- [ ] Desktop app (Electron)

---

## ğŸ¤ Contributing

We love contributions! Here's how you can help:

### Quick Contribution Guide

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing`)
3. **Commit** changes (`git commit -m 'feat: Add amazing feature'`)
4. **Push** to branch (`git push origin feature/amazing`)
5. **Open** a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/your-username/qwen-chat.git

# Install dependencies
cd qwen-chat/frontend
npm install

cd ../backend
pip install -r requirements.txt

# Start development servers
./start-servers.sh
```

### Guidelines

- âœ… Write descriptive commit messages
- âœ… Follow existing code style
- âœ… Update documentation
- âœ… Test your changes
- âœ… Keep PRs focused and small

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see [LICENSE](./LICENSE) file for details.

---

## ğŸŒŸ Acknowledgments

### Built With Love Using

- [Qwen 2.5](https://qwenlm.github.io/) - Powerful language model
- [Ollama](https://ollama.ai/) - Local LLM infrastructure
- [Vercel](https://vercel.com/) - Frontend hosting
- [Next.js](https://nextjs.org/) - React framework

### Inspiration

- [ChatGPT](https://chat.openai.com/) - Conversational AI interface
- [Claude](https://claude.ai/) - Advanced AI assistant
- [Open WebUI](https://github.com/open-webui/open-webui) - Self-hosted AI UI

---

<div align="center">

**Made with â¤ï¸ and AI**

![GitHub Stars](https://img.shields.io/github/stars/mrmoe28/qwen-chat?style=social)
![GitHub Forks](https://img.shields.io/github/forks/mrmoe28/qwen-chat?style=social)

â­ Star this repo if you find it useful!

[GitHub](https://github.com/mrmoe28/qwen-chat) â€¢ [Report Bug](https://github.com/mrmoe28/qwen-chat/issues) â€¢ [Request Feature](https://github.com/mrmoe28/qwen-chat/issues)

</div>
